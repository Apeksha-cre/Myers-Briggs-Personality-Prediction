# -*- coding: utf-8 -*-
"""MBTI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c9h02dbkUUbbmlum4gUtiq1tr1oTjHL1
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

# reading the dataset
path = "/content/drive/MyDrive/Colab Notebooks/mbti.csv"
#path = "/content/drive/MyDrive/Copy of mbti.csv"
#path = "/content/mbti.csv"
mbti_data = read_data = pd.read_csv(path)

#checking the data
mbti_data

#checking for nmissing values
mbti_data.isnull().sum()

"""There are no values missing in the data"""

# checking the number of rows and columns
mbti_data.shape

mbti_data

"""Number of rows: 8675
Number of columns: 2 
"""

#Checking for imbalance of class
mbti_data["type"].value_counts()

# plotting the graphs for classes
import matplotlib.pyplot as plt
mbti_data["type"].value_counts().plot(kind="bar")
plt.show()

"""Since the data seems imbalanced, we need to do something about it. We will convert it into 4 balanced class"""

#fetching common words used in data
import collections
from collections import Counter
copy_data = mbti_data.copy()
common_words = list(copy_data["posts"].apply(lambda x: x.split()))
common_words = [x for y in common_words for x in y]
Counter(common_words).most_common(20)

import wordcloud
from wordcloud import WordCloud
fig, ax = plt.subplots(len(copy_data['type'].unique()), sharex=True, figsize=(20,len(copy_data['type'].unique())))
k = 0
for i in copy_data['type'].unique():
    copy_data_4 = copy_data[copy_data['type'] == i]
    wordcloud = WordCloud(max_words=1000,relative_scaling=1,normalize_plurals=False,background_color="white").generate(copy_data_4['posts'].to_string())
    plt.subplot(4,4,k+1)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(i)
    ax[k].axis("off")
    k+=1

# handling imbalanced class
mbti_data["Extrovert"] = mbti_data["type"].apply(lambda a: 1 if a[0] == "E" else 0)
mbti_data["Sensing"] = mbti_data["type"].apply(lambda a: 1 if a[1] == "S" else 0)
mbti_data["Thinking"] = mbti_data["type"].apply(lambda a: 1 if a[2] == "T" else 0)
mbti_data["Judging"] = mbti_data["type"].apply(lambda a: 1 if a[3] == "J" else 0)

mbti_data = mbti_data[["type", "Extrovert", "Sensing", "Thinking", "Judging", "posts"]]
mbti_data.head()

# Printing the counts of each type
introvert_count = mbti_data["Extrovert"].value_counts()[0]
print(f"Introvert Count: {introvert_count}")
extrovert_count = mbti_data["Extrovert"].value_counts()[1]
print(f"Extrovert Count: {extrovert_count}")
intuition_count = mbti_data["Sensing"].value_counts()[0]
print(f"Intuition Count: {intuition_count}")
sensing_count = mbti_data["Sensing"].value_counts()[1]
print(f"Sensing Count: {sensing_count}")
feeling_count = mbti_data["Thinking"].value_counts()[0]
print(f"Feeling Count: {feeling_count}")
thinking_count = mbti_data["Thinking"].value_counts()[1]
print(f"Thinking Count: {thinking_count}")
perceiving_count = mbti_data["Judging"].value_counts()[0]
print(f"Perceiving Count: {perceiving_count}")
judging_count = mbti_data["Judging"].value_counts()[1]
print(f"Judging Count: {judging_count}")

# plotting graphs for 
import seaborn as sns
sns.countplot(x = 'variable', hue = 'value', data = pd.melt(mbti_data[["Extrovert", "Sensing", "Thinking", "Judging"]]))

# Checking for correlation 
correlation = mbti_data[["Extrovert", "Sensing", "Thinking", "Judging"]].corr()
correlation

"""From the above table we can see that there is no such correlations between the features"""

# plotting graph
map_correlation = plt.cm.RdBu
correlation = mbti_data[['Extrovert','Sensing','Thinking','Judging']].corr()
plt.figure(figsize=(12,10))
plt.title('Features Correlation Heatmap', size=15)
sns.heatmap(correlation, cmap=map_correlation,  annot=True, linewidths=1)

"""Data Pre-processing"""

# Lemmitzation
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('wordnet')


lmtz = WordNetLemmatizer()

# Remove the stop words for speed 
stop_words = stopwords.words('english')

# Remove these from the posts
mbti_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP', 'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']
mbti_list = [x.lower() for x in mbti_list]

mbti_data

# Splitting the MBTI personality into 4 letters and binarizing it

binary_personality = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':1, 'P':0}
binary_personality_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {1:'J', 0:'P'}]

def translate_personality(personality):
    # transform mbti to binary vector
    return [binary_personality[l] for l in personality]

#To show result output for personality prediction
def translate_back(personality):
    # transform binary vector to mbti personality
    s = ""
    for i, l in enumerate(personality):
        s += binary_personality_list[i][l]
    return s

per_list_bin = np.array([translate_personality(p) for p in mbti_data.type])
print("Binarize MBTI list: \n%s" % per_list_bin)

# Cleaning Data in the post
import re 
def pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True):
  per_list = []
  post_list = []
  len_data = len(data)
  i=0
  
  for row in data.iterrows():
      posts = row[1].posts
      temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)
      temp = re.sub("[^a-zA-Z]", " ", temp)
      temp = re.sub(' +', ' ', temp).lower()
      temp = re.sub(r'([a-z])\1{2,}[\s|\w]*', '', temp)
      if remove_stop_words:
          temp = " ".join([lmtz.lemmatize(w) for w in temp.split(' ') if w not in stop_words])
      else:
          temp = " ".join([lmtz.lemmatize(w) for w in temp.split(' ')])
          
      if remove_mbti_profiles:
          for t in mbti_list:
              temp = temp.replace(t,"")

      labelized_type = translate_personality(row[1].type)
      per_list.append(labelized_type)
      post_list.append(temp)

  post_list = np.array(post_list)
  per_list = np.array(per_list)
  return post_list, per_list

post_list, per_list  = pre_process_text(mbti_data, remove_stop_words=True, remove_mbti_profiles=True)

nRow, nCol = per_list.shape
print(f'No. of posts = {nRow}  and No. of Personalities = {nCol} ')

# Vectorizing the database posts to a matrix of token counts for the model
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer


count_vectorizer = CountVectorizer(analyzer="word", max_features=1000,  max_df=0.7, min_df=0.1) 
print("Using CountVectorizer :")
X_cnt = count_vectorizer.fit_transform(post_list)
print(X_cnt.shape)
# For the Standardization or Feature Scaling Stage :-
# Transform the count matrix to a normalized tf or tf-idf representation
tf_idf_vectorizer = TfidfTransformer()

# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation
print("\nUsing Tf-idf :")

print("Now the dataset size is as below")
X_td =  tf_idf_vectorizer.fit_transform(X_cnt)
print(X_td.shape)

mbti_type = [ "IE: Introversion (I) / Extroversion (E)", "NS: Intuition (N) / Sensing (S)", 
                   "FT: Feeling (F) / Thinking (T)", "JP: Judging (J) / Perceiving (P)"  ]

## import models
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.svm import LinearSVC, SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

# model evaluation
from imblearn.metrics import classification_report_imbalanced
from imblearn.metrics import geometric_mean_score
from sklearn.metrics import average_precision_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit

## list accuracies to plot graph
IE_accuracies = []
NS_accuracies = []
FT_accuracies = []
JP_accuracies = []

from imblearn.over_sampling import SMOTE, RandomOverSampler

per_list
o = pd.DataFrame(per_list)
o.rename(columns={0:"Extrovert", 1 :"Sensing", 2:"Thinking", 3:"Judging"}, inplace = True)
o

def model(model, param=None):
  for l in range(4):
    X = X_td
    Y = per_list[:,l]

    X_ros, Y_ros = RandomOverSampler(random_state=42, sampling_strategy="minority").fit_resample(X,Y)
    # split data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X_ros, Y_ros, test_size=0.33, random_state=7)
   # X_train, y_train = ros.fit_sample(X_train, y_train)
    #X_test, y_test = ros.fit_sample(X_test, y_test)
    
    # fit model on training data
    if(model == 'KNeighborsClassifier'):
      model = KNeighborsClassifier(n_neighbors = param)
    elif(model == 'LogisticRegression'):
      model = LogisticRegression() 
    elif(model == 'SVC'):
      model = SVC(random_state = param, probability=True)
    elif(model == 'RandomForestClassifier'):
      model = RandomForestClassifier()

    model.fit(X_train, y_train)

    # make predictions for test data
    y_pred = model.predict(X_test)

    # y_probability 
    y_proba = model.predict_proba(X_test)[:, 1]
    
    predictions = [round(value) for value in y_pred]
    
    # evaluate predictions
    accuracy = accuracy_score(y_test, predictions)
    
    cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=27)
    scores = cross_val_score(model, X, Y, cv=cv)
    print("Cross validation results")
    cvp=scores.mean()*100
    print("%s has %0.2f%% accuracy with a standard deviation of %0.2f" % (mbti_type[l],cvp, scores.std()))
    print("\n")

    
    # precision recall score
    average_precision = average_precision_score(y_test, y_proba)
    # model evaluation
    if(mbti_type[l] == 'IE: Introversion (I) / Extroversion (E)'):
      IE_accuracies.append(accuracy * 100.0)
    if(mbti_type[l] == 'NS: Intuition (N) / Sensing (S)'):
      NS_accuracies.append(accuracy * 100.0)
    if(mbti_type[l] == 'FT: Feeling (F) / Thinking (T)'):
      FT_accuracies.append(accuracy * 100.0)
    if(mbti_type[l] == 'JP: Judging (J) / Perceiving (P)'):
      JP_accuracies.append(accuracy * 100.0)
    print("%s Accuracy: %.2f%%" % (mbti_type[l], accuracy * 100.0))
    print(
        f"Geometric Mean Score: {geometric_mean_score(y_test, y_pred, average='weighted'):.2f}"
    )
    print(f"ROC-AUC Score: {roc_auc_score(y_test, y_proba):.2f}")
    print(f"Average Precision-Recall Score: {average_precision:.2f}")
    print(classification_report_imbalanced(y_test, y_pred))

    #Generate the confusion matrix
    cf_matrix = confusion_matrix(y_test, y_pred)
    print("\nConfusion Matrix: ")
    print(cf_matrix)
    print("\n")

    fig, ax = plt.subplots(figsize=(2.5, 2.5))
    ax.matshow(cf_matrix, cmap=plt.cm.Blues, alpha=0.3)
    for i in range(cf_matrix.shape[0]):
      for j in range(cf_matrix.shape[1]):
        ax.text(x=j, y=i, s=cf_matrix[i, j], va='center', ha='center')
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.tight_layout()
    plt.show()
    print("\n")

"""# testing models"""

model('KNeighborsClassifier',param = 2)

model('LogisticRegression')



model('SVC', param = 1)

model('RandomForestClassifier')

# printing accuracies for IE class for differnt models
IE_accuracies

NS_accuracies

FT_accuracies

JP_accuracies

import matplotlib.pyplot as plt

plt.bar(["KNN", "LogisticRegression", "SVC", "RandomForest"],IE_accuracies)

plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Introvert-Extrovert Accuracy Graph')

plt.show()

plt.bar(["KNN", "LogisticRegression", "SVC", "RandomForest"],NS_accuracies)

plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Intuition-Sensing Accuracy Graph')

plt.show()

plt.bar(["KNN", "LogisticRegression", "SVC", "RandomForest"],FT_accuracies)

plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Feeling-Thinking Accuracy Graph')

plt.show()

plt.figure(figsize=(10,5))
plt.bar(["KNN", "LogisticRegression", "SVC", "RandomForest"],JP_accuracies)


plt.yticks(np.arange(0,100,5))
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Judging-Perceiving Accuracy Graph')

plt.show()

